import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import platform

import matplotlib.font_manager as fm
import platform

# í°íŠ¸ ì„¤ì • í•¨ìˆ˜
def set_korean_font():
    system_name = platform.system()
    
    if system_name == 'Darwin': # Mac
        plt.rc('font', family='AppleGothic')
    elif system_name == 'Windows': # Windows
        plt.rc('font', family='Malgun Gothic')
    else: # Linux (Streamlit Cloud í¬í•¨)
        # 1ë‹¨ê³„ì—ì„œ ì„¤ì¹˜í•œ ë‚˜ëˆ” ê³ ë”•ì„ ì„¤ì •
        try:
            # í°íŠ¸ ê²½ë¡œë¥¼ ì§ì ‘ ì§€ì •í•˜ê±°ë‚˜ í°íŠ¸ ì´ë¦„ìœ¼ë¡œ ì„¤ì •
            plt.rc('font', family='NanumGothic')
        except:
            # ë§Œì•½ í°íŠ¸ ì´ë¦„ìœ¼ë¡œ ì•ˆë  ê²½ìš°ë¥¼ ëŒ€ë¹„
            st.warning("ë‚˜ëˆ” í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
    plt.rcParams['axes.unicode_minus'] = False

# ì•± ì‹œì‘ ë¶€ë¶„ì—ì„œ ì‹¤í–‰
set_korean_font()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_curve, auc, confusion_matrix
from sklearn.feature_selection import SequentialFeatureSelector

# SMOTE ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ (ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬)
try:
    from imblearn.over_sampling import SMOTE
    has_smote = True
except ImportError:
    has_smote = False
    st.warning("imbalanced-learn ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. SMOTE ê¸°ëŠ¥ì€ ê±´ë„ˆëœë‹ˆë‹¤. (ì„¤ì¹˜ ëª…ë ¹ì–´: pip install imbalanced-learn)")

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ì•±", layout="wide")

# ì œëª©
st.title("ğŸ“Š ê³ ê° ì´íƒˆ ê´€ë¦¬ ë°ì´í„° ë¶„ì„ ì•±")

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
@st.cache_data
def load_data():
    # íŒŒì¼ì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆë‹¤ê³  ê°€ì •
    df = pd.read_csv("Churn_management.csv")
    return df

try:
    df = load_data()
    st.success("ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤!")
except FileNotFoundError:
    st.error("'Churn_management.csv' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°™ì€ í´ë”ì— ìœ„ì¹˜ì‹œì¼œ ì£¼ì„¸ìš”.")
    st.stop()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ìš©)
if 'processed_data' not in st.session_state:
    st.session_state.processed_data = None
if 'X_train' not in st.session_state:
    st.session_state.X_train = None

# ==========================================
# 1. ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”
# ==========================================
st.header("1. ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”")

# ì›ë³¸ ë°ì´í„° ë³´ê¸°
if st.checkbox("ì›ë³¸ ë°ì´í„° ë³´ê¸°"):
    st.dataframe(df.head())

# ì¢…ì† ë³€ìˆ˜ ë¶„í¬ í™•ì¸
st.subheader("ì¢…ì† ë³€ìˆ˜ ë¶„í¬ (ì´íƒˆ ì—¬ë¶€: Exited)")
fig_target, ax_target = plt.subplots(figsize=(6, 4))
sns.countplot(x='Exited', data=df, ax=ax_target, palette='viridis')
ax_target.set_title("ì´íƒˆ ì—¬ë¶€ ë¶„í¬ (0: ìœ ì§€, 1: ì´íƒˆ)")
st.pyplot(fig_target)

# ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™”
st.subheader("ë³€ìˆ˜ë³„ ì‹œê°í™”")
col1, col2, col3 = st.columns(3)

with col1:
    plot_type = st.selectbox("ê·¸ë˜í”„ ìœ í˜• ì„ íƒ", ["íˆìŠ¤í† ê·¸ë¨", "ë°•ìŠ¤ í”Œë¡¯", "ì‚°ì ë„", "ë§‰ëŒ€ ì°¨íŠ¸", "ì„  ì°¨íŠ¸"])
with col2:
    x_var = st.selectbox("Xì¶• ë³€ìˆ˜ ì„ íƒ", df.columns)
with col3:
    y_var = st.selectbox("Yì¶• ë³€ìˆ˜ ì„ íƒ (íˆìŠ¤í† ê·¸ë¨/ë°•ìŠ¤í”Œë¡¯ì€ ì„ íƒ ì‚¬í•­)", [None] + list(df.columns))

fig_custom, ax_custom = plt.subplots(figsize=(8, 5))

try:
    if plot_type == "íˆìŠ¤í† ê·¸ë¨":
        sns.histplot(data=df, x=x_var, kde=True, ax=ax_custom)
    elif plot_type == "ë°•ìŠ¤ í”Œë¡¯":
        sns.boxplot(data=df, x=x_var, y=y_var, ax=ax_custom)
    elif plot_type == "ì‚°ì ë„":
        if y_var:
            sns.scatterplot(data=df, x=x_var, y=y_var, hue='Exited', ax=ax_custom)
        else:
            st.warning("ì‚°ì ë„ë¥¼ ê·¸ë¦¬ë ¤ë©´ Yì¶• ë³€ìˆ˜ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
    elif plot_type == "ë§‰ëŒ€ ì°¨íŠ¸":
        if y_var:
            sns.barplot(data=df, x=x_var, y=y_var, ax=ax_custom)
        else:
            sns.countplot(data=df, x=x_var, ax=ax_custom)
    elif plot_type == "ì„  ì°¨íŠ¸":
        if y_var:
            sns.lineplot(data=df, x=x_var, y=y_var, ax=ax_custom)
        else:
            st.warning("ì„  ì°¨íŠ¸ë¥¼ ê·¸ë¦¬ë ¤ë©´ Yì¶• ë³€ìˆ˜ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
    
    st.pyplot(fig_custom)
except Exception as e:
    st.error(f"ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


# ==========================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬
# ==========================================
st.header("2. ë°ì´í„° ì „ì²˜ë¦¬")

if st.button("ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰"):
    with st.spinner("ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì„ íƒ ì¤‘..."):
        df_processed = df.copy()

        # 1. ë¶ˆí•„ìš”í•œ ì‹ë³„ì ì»¬ëŸ¼ ì œê±°
        drop_cols = ['id', 'CustomerId', 'Surname']
        df_processed = df_processed.drop(columns=[c for c in drop_cols if c in df_processed.columns])
        
        # 2. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ìˆ˜ì¹˜í˜•: í‰ê· , ë²”ì£¼í˜•: ìµœë¹ˆê°’)
        num_cols = df_processed.select_dtypes(include=['float64', 'int64']).columns
        cat_cols = df_processed.select_dtypes(include=['object']).columns
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬ (Exitedê°€ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— í¬í•¨ë˜ì§€ ì•Šë„ë¡)
        if 'Exited' in num_cols:
            num_cols = num_cols.drop('Exited')

        imputer_num = SimpleImputer(strategy='mean')
        df_processed[num_cols] = imputer_num.fit_transform(df_processed[num_cols])
        
        imputer_cat = SimpleImputer(strategy='most_frequent')
        df_processed[cat_cols] = imputer_cat.fit_transform(df_processed[cat_cols])

        # 3. ì´ìƒì¹˜ ì²˜ë¦¬ (IQR ë°©ì‹ - Clipping)
        for col in num_cols:
            Q1 = df_processed[col].quantile(0.25)
            Q3 = df_processed[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            df_processed[col] = np.clip(df_processed[col], lower_bound, upper_bound)

        # 4. ì›-í•« ì¸ì½”ë”© (One-Hot Encoding)
        df_processed = pd.get_dummies(df_processed, columns=cat_cols, drop_first=True)

        # 5. íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ (StandardScaler)
        scaler = StandardScaler()
        cols_to_scale = [c for c in df_processed.columns if c != 'Exited']
        df_processed[cols_to_scale] = scaler.fit_transform(df_processed[cols_to_scale])

        # X, y ë¶„ë¦¬
        X = df_processed.drop('Exited', axis=1)
        y = df_processed['Exited']

        # 6. í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (SMOTE)
        if has_smote:
            smote = SMOTE(random_state=42)
            X, y = smote.fit_resample(X, y)
        
        # 7. ë‹¨ê³„ì  ì„ íƒë²• (Stepwise Selection)
        # ì†ë„ë¥¼ ìœ„í•´ ë¡œì§€ìŠ¤í‹± íšŒê·€ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        sfs = SequentialFeatureSelector(LogisticRegression(max_iter=1000), 
                                        n_features_to_select='auto', 
                                        direction='forward',
                                        tol=None)
        sfs.fit(X, y)
        selected_features = X.columns[sfs.get_support()]
        X = X[selected_features]
        
        # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state.processed_data = {'X': X, 'y': y}
        st.success("ì „ì²˜ë¦¬ ì™„ë£Œ! ëª¨ë¸ë§ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.write(f"**ì„ íƒëœ íŠ¹ì„±:** {list(selected_features)}")
        st.write(f"**ì „ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°:** {X.shape}")


# ==========================================
# 3. ë°ì´í„° ë¶„í•  ë° ëª¨ë¸ ì„¤ì •
# ==========================================
st.header("3. ë°ì´í„° ë¶„í•  ë° ëª¨ë¸ ì„¤ì •")

if st.session_state.processed_data is not None:
    X = st.session_state.processed_data['X']
    y = st.session_state.processed_data['y']

    # ë¶„í•  ë¹„ìœ¨ ì„ íƒ
    split_ratio = st.radio("í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  ë¹„ìœ¨ ì„ íƒ", ["7:3", "8:2"])
    test_size = 0.3 if split_ratio == "7:3" else 0.2

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    col_dt, col_log = st.columns(2)

    with col_dt:
        st.subheader("ì˜ì‚¬ê²°ì •ë‚˜ë¬´(Decision Tree) ì˜µì…˜")
        dt_max_depth = st.slider("ìµœëŒ€ ê¹Šì´ (Max Depth)", 1, 20, 5)
        dt_criterion = st.selectbox("ë¶„í•  ê¸°ì¤€ (Criterion)", ["gini", "entropy"])

    with col_log:
        st.subheader("ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression) ì˜µì…˜")
        lr_C = st.slider("C ê°’ (ê·œì œ ê°•ë„ì˜ ì—­ìˆ˜)", 0.01, 10.0, 1.0)
        lr_max_iter = st.number_input("ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (Max Iterations)", value=500)

    # í•™ìŠµ ë° í‰ê°€ ë²„íŠ¼
    if st.button("ëª¨ë¸ í•™ìŠµ ë° í‰ê°€"):
        
        # í‰ê°€ì§€í‘œ ì¶œë ¥ í—¬í¼ í•¨ìˆ˜
        def show_metrics(y_true, y_pred, y_prob, title):
            acc = accuracy_score(y_true, y_pred)
            prec = precision_score(y_true, y_pred)
            rec = recall_score(y_true, y_pred)
            f1 = f1_score(y_true, y_pred)
            
            st.markdown(f"### **{title} í‰ê°€ ì§€í‘œ**")
            m1, m2, m3, m4 = st.columns(4)
            m1.metric("ì •í™•ë„ (Accuracy)", f"{acc:.2f}")
            m2.metric("ì •ë°€ë„ (Precision)", f"{prec:.2f}")
            m3.metric("ì¬í˜„ìœ¨ (Recall)", f"{rec:.2f}")
            m4.metric("F1 ì ìˆ˜", f"{f1:.2f}")

            # ê·¸ë˜í”„ ì¶œë ¥
            c1, c2 = st.columns(2)
            
            # Confusion Matrix
            with c1:
                st.write("**í˜¼ë™ í–‰ë ¬ (Confusion Matrix)**")
                cm = confusion_matrix(y_true, y_pred)
                fig_cm, ax_cm = plt.subplots()
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax_cm)
                ax_cm.set_xlabel("ì˜ˆì¸¡ê°’")
                ax_cm.set_ylabel("ì‹¤ì œê°’")
                st.pyplot(fig_cm)
            
            # ROC Curve
            with c2:
                st.write("**ROC ê³¡ì„ **")
                fpr, tpr, _ = roc_curve(y_true, y_prob)
                roc_auc = auc(fpr, tpr)
                fig_roc, ax_roc = plt.subplots()
                ax_roc.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
                ax_roc.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
                ax_roc.set_xlim([0.0, 1.0])
                ax_roc.set_ylim([0.0, 1.05])
                ax_roc.set_xlabel('False Positive Rate (ìœ„ì–‘ì„±ë¥ )')
                ax_roc.set_ylabel('True Positive Rate (ì§„ì–‘ì„±ë¥ )')
                ax_roc.legend(loc="lower right")
                st.pyplot(fig_roc)

        # ==========================================
        # 4. ëª¨ë¸ í‰ê°€ (ì˜ì‚¬ê²°ì •ë‚˜ë¬´)
        # ==========================================
        st.header("4. ëª¨ë¸ í‰ê°€ - ì˜ì‚¬ê²°ì •ë‚˜ë¬´")
        dt_model = DecisionTreeClassifier(max_depth=dt_max_depth, criterion=dt_criterion, random_state=42)
        dt_model.fit(X_train, y_train)
        y_pred_dt = dt_model.predict(X_test)
        y_prob_dt = dt_model.predict_proba(X_test)[:, 1]
        
        show_metrics(y_test, y_pred_dt, y_prob_dt, "ì˜ì‚¬ê²°ì •ë‚˜ë¬´")

        st.markdown("---") # êµ¬ë¶„ì„ 

        # ==========================================
        # 5. ëª¨ë¸ í‰ê°€ (ë¡œì§“ ëª¨ë¸)
        # ==========================================
        st.header("5. ëª¨ë¸ í‰ê°€ - ë¡œì§€ìŠ¤í‹± íšŒê·€")
        lr_model = LogisticRegression(C=lr_C, max_iter=int(lr_max_iter), random_state=42)
        lr_model.fit(X_train, y_train)
        y_pred_lr = lr_model.predict(X_test)
        y_prob_lr = lr_model.predict_proba(X_test)[:, 1]

        show_metrics(y_test, y_pred_lr, y_prob_lr, "ë¡œì§€ìŠ¤í‹± íšŒê·€")

else:
    st.info("ğŸ‘† ë¨¼ì € 2ë²ˆ ì„¹ì…˜ì˜ 'ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
